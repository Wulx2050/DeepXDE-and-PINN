# 分数阶微积分

简单来说，经典的微积分运算是指整数阶的微分 $D^n$ 或积分 $I^n$，分数阶微积分就是将整数阶的微积分运算推广到分数阶的微分或积分运算。这里的“分数阶”不仅仅指的是有理分数，也包括阶数为无理数和复数的情形。
$$
\large
\begin{aligned}
I^nf(t) &= \frac{1}{(n-1)!}\int_0^t (t-\tau)^{n-1}f(\tau) d\tau, &n\in\mathbb{N}^+,\\

D^{-\mu} f(t) &= I^{\mu} f(t) = \frac{1}{\Gamma(\mu)}\int_0^t (t-\tau)^{\mu-1}f(\tau) d\tau, &\Re(\mu)>0, \\

D^{-n} f(t) &= \frac{1}{\Gamma(n)} \frac{d^n}{dt^n} \left( \int_0^t (t-\tau)^{n-1}f(\tau) d\tau \right)  ,&n\in\mathbb{N}^+,\\

D^{\mu} f(t) &= \frac{(-1)^n}{\Gamma(n-\mu)} \frac{d^n}{dt^n}\left( \int_0^t (\tau-t)^{n-\mu-1}u(\tau) d\tau \right), &\Re(\mu)>0, n=\lceil \mu \rceil. \\

\end{aligned}\\
$$




### 常用函数

$$
\large
\begin{aligned}
\text{Gamma函数: }&\\
\Gamma (z) &= \int_0^{\infty} e^{-t}z^{z-1}dt,   &\Re(z)>0\\
    &=\lim_{n\to\infty} \frac{n!n^z}{z(z+1)\dots(z+n)}
    =\frac{1}{z}\prod^{\infty}_{k=0} \frac{(1+\frac{1}{k})^z}{1+\frac{z}{k}}. &(z\neq-n)\\
    
\text{Beta函数: }\\
B(p,q) &= \int_0^1 \tau^{p-1}(1-\tau)^{q-1}d\tau,\Re(p)>0,\Re(q)>0.\\

\text{Laplace变换: }\mathbf{L}\\
F(s) &= \int_0^{+\infty} f(t)e^{-st}dt. &s=\beta+i\omega, t\in[0,+\infty)\\
\text{Laplace逆变换: }\mathbf{L}^{-1}\\
f(t) &= \frac{1}{2\pi i}\int_{\beta-i\infty}^{\beta+i\infty} F(s)e^{st}ds. &t>0,\Re(s)>c\\

\text{Fourier变换: }\mathbf{F}\\
F(\omega) &= \int_{-\infty}^{+\infty} f(\tau)e^{-i\omega \tau}d\tau.\\
\text{Fourier逆变换: }\mathbf{F}^{-1}\\
f(t) &= \frac{1}{2\pi}\int_{-\infty}^{+\infty} F(\omega)e^{i\omega t}d\omega.\\

\text{Mittag-Leffler函数: }\\
E_{\alpha,\beta}(z) &= \sum_{k=0}^{\infty} \frac{z^k}{\Gamma(k\alpha+\beta)}. &\alpha>0, \beta>0, z\in\mathbf{C} \\

\end{aligned}\\
$$



分数阶积分:
$$
\large 
\begin{aligned}
&&&&&&&&&&&&&&&&&&&&&&&&&&&&\\
Gr\ddot{u}nwald-Letnikov&型分数阶积分: \\
&{_{\,\,\,\,a}^{GL}}D^{-\mu}_t u(t) = \frac{1}{\Gamma(\mu)}\int_a^t (t-\tau)^{\mu-1}u(\tau) d\tau, &\Re(\mu) >0. \\
或者换几个字母: \\
&{_{\,\,\,\,c}}D^{-\nu}_x f(x) = \frac{1}{\Gamma(\nu)}\int_c^x (x-t)^{\nu-1}f(t) dt, &\Re(\nu) >0. \\

Riemann-Liouvile&型分数阶积分,a或c=0: \\
&{_{\,\,\,\,0}}D^{-\nu}_x f(x) = \frac{1}{\Gamma(\nu)}\int_0^x (x-t)^{\nu-1}f(t) dt, &\Re(\nu) >0. \\

Caputo型分数阶积分: \\
&没有定义.\\

Weyl型分数阶积分:\\
&{_{\,\,\,\,x}}W^{-\nu}_{\infty} f(x) = \frac{1}{\Gamma(\nu)}\int_x^{\infty} (t-x)^{\nu-1}f(t) dt, &\Re(\nu) >0. \\

\end{aligned}\\
$$


### $$\text{Gr$\ddot{u}$nwald-Letnikov}$$ 型分数阶微积分

$$
\large 
\begin{aligned}
\text{G-L型分数阶导数: }\\
{_{\,\,\,\,a}^{GL}}D^{\mu}_t u(t) &= \lim_{h\to0} u^{(\mu)}_h (t)
\overset{\text{def}}{=} \lim_{h\to 0^+, nh=t-a} h^{-\mu} \sum^n_{i=0}
\begin{bmatrix} -\mu\\i \end{bmatrix} u(t-ih), \\
&= \sum^m_{k=0}\frac{u^{(k)}(a)(t-a)^{-\mu+k}}{\Gamma(-\mu+k+1)} +\frac{1}{\Gamma(-\mu+m+1)} 
\int_a^t (t-\tau)^{-\mu+m} u^{(m+1)}(\tau) d\tau .\\

\text{G-L型分数阶积分: }\\
{_{\,\,\,\,a}^{GL}}D^{-\mu}_t u(t) & \overset{\text{def}}{=} \lim_{h\to 0, nh=t-a}h^{\mu} \sum^n_{i=0}
\begin{bmatrix} -\mu\\i \end{bmatrix} u(t-ih), \\
&= \frac{1}{\Gamma(\mu)} \int_a^t (t-\tau)^{\mu-1} u(\tau) d\tau,\\
&= \sum^m_{k=0}\frac{u^{(k)}(a)(t-a)^{\mu+k}}{\Gamma(\mu+k+1)} +\frac{1}{\Gamma(\mu+m+1)} 
\int_a^t (t-\tau)^{\mu+m} u^{(m+1)}(\tau) d\tau.\\\\

其中\begin{bmatrix} -\mu\\i \end{bmatrix} &=  \frac{(-\mu)\bullet(-\mu+1) \bullet (-\mu+2) \bullet \dots \bullet (-\mu+i-1)} {i!}, \\
&t\in(a,b),\mu\in\R^+, m至少取到[\mu],\mu\in[m,m+1). \\

\end{aligned}\\
$$

G-L型不常用，目前常用的是 R-L，Caputo, Weyl型导数的定义。



### $\text{Riemann-Liouvile}$ 型分数阶微积分

$$
\large \begin{aligned}
&&&&&&\\
\text{左R-L型分数阶积分:}\\
\sideset{_a}{_t^{-\mu}}D u(t) &= \frac{1}{\Gamma(\mu)}\int_a^t (t-\tau)^{\mu-1}u(\tau) d\tau. \\

\text{左R-L型分数阶导数:}\\
\sideset{_a}{_t^{\mu}}D u(t) &= D^n[\sideset{_a}{_t^{-(n-\mu)}}D u(t)]  \\
&= \frac{(-1)^n}{\Gamma(n-\mu)} \frac{d^n}{dt^n}\left( \int_a^t (\tau-t)^{n-\mu-1}u(\tau) d\tau \right). \\

\text{右R-L型分数阶积分:}\\
\sideset{_t}{_b^{-\mu}}D u(t) &= \frac{1}{\Gamma(\mu)}\int_t^b (\tau-t)^{\mu-1}u(\tau) d\tau. \\

\text{右R-L型分数阶导数:}\\
\sideset{_t}{_b^{\mu}}D u(t) &= (-D)^n[\sideset{_t}{_b^{\mu-n}}D u(t)]  \\
&= \frac{(-1)^n}{\Gamma(n-\mu)} \frac{d^n}{dt^n}\left( \int_t^b (\tau-t)^{n-\mu-1}u(\tau) d\tau \right). \\
其中\mu>0, n=\lceil \mu & \rceil, t\in(a,b). \\
\end{aligned}\\
$$

R-L型对G-L定义进行了改进，是G-L型的扩充。R-L型可以简化分数阶微积分的计算过程，是应用较为广泛的一种。





### $\text{Caputo}$ 型分数阶微积分

$$
\large \begin{aligned}
t\in(a,b),\mu>0,n&=\lceil \mu \rceil, n-1<\mu \leq n, t>a, u^{(n)}是函数u的n阶导数.\\

\text{左Caputo型分数阶导数:}\\
定义1：
\sideset{_a^C}{_t^{\mu}}D u(t) &= \sideset{_a}{_t^{\mu-n}}D  D^n u(t) 
    =\frac{1}{\Gamma(n-\mu)} \int_a^t (t-\xi)^{n-\mu-1} u^{(n)}(\xi)d\xi \\
    &=\frac{u^{(n)}(a)(t-a)^{n-\mu}}{\Gamma(n-\mu+1)} + \frac{1}{\Gamma(n-\mu+1)} \int_a^t (t-\xi)^{n-\mu} u^{(n+1)}(\xi) d\xi .\\
定义2：
(\sideset{_a^C}{_t^{\mu}}D u)(t) &= \left( \sideset{_a}{_t^{\mu}}D  \left[u(t) - \sum_{k=0}^{n-1} \frac{u^{(k)}(a)}{k!} (t-a)^k \right] \right) (t).\\

\text{右Caputo型分数阶导数:}\\
定义1：
\sideset{_t^C}{_b^{\mu}}D u(t) &= \sideset{_t}{_b^{\mu-n}}D  (-D)^n u(t) \\
    &=\frac{1}{\Gamma(n-\mu)} \int_t^b (\xi-t)^{n-\mu-1} (-1)^n u^{(n)}(\xi)d\xi .\\

定义2：
(\sideset{_t^C}{_b^{\mu}}D u)(t) &= \left( \sideset{_t}{_b^{\mu}}D  \left[ u(t) - \sum_{k=0}^{n-1} \frac{u^{(k)}(b)}{k!} (b-t)^k \right] \right) (t).\\

\text{左Caputo型分数阶积分:}\\
没有定义.&或者说与R-L相同\\

\text{右Caputo型分数阶积分:}\\
没有定义.&或者说与R-L相同\\

\end{aligned}\\
$$



与R-L型定义相比，Caputo型定义将对函数 $u$ 的整数阶导数放进积分内，改为对变量 $\xi$ 的导数。

在许多的物理、力学等实际问题的数学建模及求解过程中，更多地选择Caputo型导数定义。

R-L型分数阶微分可以简化分数阶导数的计算；Caputo型分数阶导数让其Lapace变化更简洁，有利于分数阶微分方程的求解与分析。Caputo型分数阶导数的优越性在于分数阶微分系统的初始条件上，定义采取了与整数阶微分方程相同的形式，包括整数阶导数值对于未知函数在端点t=a的值的限制等等，而R-L型分数阶微分不具有上述好的特点。







### $\text{Weyl}$ 型分数阶微积分

$$
\large 
\begin{aligned}
&&&&&&\\
\text{Weyl型分数阶积分: }\\
{_t}W^{-\mu}_{\infty} u(t) &= W^{-\mu} u(t) = \frac{1}{\Gamma(\mu)}\int_t^{\infty} (\tau-t)^{\mu-1}u(\tau) d\tau,  \\
记E=-D=-\frac{d}{dt}&,则E^n = (-1)^n D^n,\\
\text{Weyl型分数阶导数: }\\
W^{\mu} u(t) &= E^n[W^{-(n-\mu)} u(t)],  \\
   &= \frac{(-1)^n}{\Gamma(n-\mu)} \frac{d^n}{dt^n} \left( \int_t^{\infty} (\tau-t)^{n-\mu-1}u(\tau) d\tau \right)\\

其中 \Re(\mu) >0,n&=[\mu]+1, t>0,u\in S(\mathbf{R}^n) 即速降函数.\\
\end{aligned}\\
$$

形式上，右R-L型令$b=\infty$与Weyl型是一致的。Weyl型应用于分形曲线建模。



参考文献：

[1]吴强,黄建华.分数阶微积分[M].清华大学出版社.2016.5.ISBN 978-7-302-43546-4.

[2]郭柏灵,蒲学科,黄凤辉.分数阶偏微分方程及其数值解[M].科学出版社.2011.11.ISBN 978-7-03-032684-3.




$$
\sideset{_1^2}{_3^4}X_a^b,
$$



# 分数阶网络信息神经网络 fPINN

Physics-Informed Neural Networks (PINN) 是一种神经网络 (NNs)，它可以将模型方程(model equations，主要是非线性微分模型方程)编码为神经网络的一个组成部分。物理信息神经网络的理论非常简单：在训练神经网络的时候，直接把已知的微分方程加入到损失函数中。PINN 可以在减少微分方程残差的同时，拟合观察数据，是一种多任务学习框架。

PINN 被用来求解偏微分方程、随机偏微分方程、分数阶微分方程和积分微分方程的正问题和逆问题(data-driven solution, data-driven discovery)。

正问题是给出微分方程和定解条件，来解出微分方程的未知函数。逆问题是给出含参微分方程、定解条件和未知函数的某些值，求解微分方程中的未知参数；或者给出微分方程、部分已知条件和未知函数的某些值，去求未知条件。(定解条件definite condition=边界条件boundary condition+初始条件initial condition)

正问题：由微分方程描述的某种物理过程或现象，并根据系统的状态变量的某些特定条件来确定整个系统的状态变量的变化规律。

逆问题：又称为反问题，指根据系统的可观测量来探求整个系统的状态变量的变化规律或系统所受的外部影响。



(初始条件=初值条件=初始值条件=initial condition)

(边界条件是控制方程有确定解的前提，对于任何问题，都需要给定边界条件)



微分方程可以描述系统的状态变量的变化规律，许多自然规律都可以用微分方程表述的。

理查德·费曼说：只有一种精确的方法能够表述物理定律，就是使用微分方程。[[There is only one precise way of presenting the laws, and that is by means of differential equations.]](https://www.feynmanlectures.caltech.edu/II_02.html)

在过去的几百年里，微分方程主要是基于数学或物理原理推导而成的，比如牛顿万有引力公式、爱因斯坦引力场方程和薛定谔波动方程；同时也有一些是靠数据和经验拼出来的，比如开普勒三大公式和普朗克黑体辐射公式。

神经网络不懂得数学定理和物理定律，经典的神经网络只能单纯通过数据来拟合微分方程的解，效率太低，而 PINN 可以同时利用数据和微分方程来拟合微分方程的解。

近二十年来，随着传感器、数据存储技术和计算能力的飞速发展，大量的数据可以很容易地被收集、存储和处理。大数据+计算能力+神经网络=深度学习。

神经网络是一个相互连接的神经元网络，每个神经元都是一个有限函数逼近器。这样，神经网络被视为通用函数逼近器。

神经网络是一个多义词，既可以指人工神经网络，也可以指生物神经网络——人脑的神经网络。人工神经网络按其模型结构可以分为前馈型网络（也称为多层感知机）和反馈型网络（也称为Hopfield网络）两大类，前者在数学上可以看作是一类大规模的非线性映射系统，后者则是一类大规模的非线性动力学系统。一般来说，当我们提到神经网络时，是指人工神经网络。

前馈神经网络(feedforward neural network, FNN)，是一种最简单的神经网络，各神经元分层排列，每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层，各层间没有反馈。是应用最广泛、发展最迅速的人工神经网络之一。

前馈神经网络(feedforward neural network, FNN)，是一种最简单的神经网络，是人工神经网络的前馈型网络的一种。前馈神经网络采用一种单向多层结构，其中每一层包含若干个神经元，同层神经元之间没有连接，只能接收前一层神经元的信号，并产生输出传到下一层。一个n(>=1)层的神经网络，第0层叫输入层(输入层神经元不进行函数处理，所以输入层不计入层数)，最后一层(第n层)叫输出层，其他中间层(1 to n-1 层)叫做隐藏层。隐藏层可以是一层，也可以是多层。

整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示。

![img](https://bkimg.cdn.bcebos.com/pic/8601a18b87d6277f8394ed1327381f30e924fc13?x-bce-process=image/resize,m_lfit,w_1280,limit_1/format,f_auto)

使用深度学习方法求解PDE，面临的主要问题是求解高维微分方程问题和更好的处理间断，绝大多数工作使用的方法都是监督学习，目前来看无监督学习方法和强化学习方法效果还是不够理想。



为什么 $t\in[0,1]$ 神经网络拟合的很好，但是 $t\in[0,100]$ 神经网络拟合的就不好了。

与网络的深度没有太大关系；**与网络的宽度有关**，-1层的神经元数量需要特别多。

“肥胖”网络的隐藏层数较少。虽然有研究表明，浅而肥的网络也可以拟合任何的函数，但它需要非常的“肥胖”，可能一层就要成千上万个神经元。而这直接导致的后果是参数的数量增加到很多很多。

也有实验表明，也就是上图的实验，我们可以清楚的看出，当准确率差不多的时候，参数的数量却相差数倍。这也说明我们一般用深层的神经网络而不是浅层“肥胖”的网络。

但是拟合复杂函数需要必要的宽度。



神经网络的拟合顺序是先小再大，先低频再高频。

**不使用数字表示数，而是用字符串表示数。**



![nn.gif (923×322)](https://benmoseley.blog/wp-content/uploads/2021/08/nn.gif)





![img](https://benmoseley.blog/wp-content/uploads/2021/08/pinn.gif)



一个神经元包含两个部分：一是对输入加权求再加上偏置(线性变换)，二是激活函数对求和结果的激活或者抑制(非线性激活函数施加的非线性变换)。注意生物神经元要复杂的多，其中一个方面是生物神经元的输出是一个脉冲，而现在大多数的人工神经元输出的就是一个值。现在有一些脉冲人工神经网络就是来模仿生物神经元的脉冲输出。



Jeff Dean在论文中指出了机器学习研究社区正在兴起的研究领域：

**稀疏激活模型**，比如稀疏门控专家混合模型（sparsely-gated mixture of expertsE）展示了如何构建非常大容量的模型，其中对于任何给定的实例只有一部分模型被激活，比如包括2048个专家，其中激活2-3个。

**自动化机器学习**（AutoML），其中神经架构搜索（NAS）或进化架构搜索（EAS）等技术可以自动学习 ML 模型或组件的高效结构或其他方面以对给定任务的准确率进行优化。AutoML 通常涉及运行很多自动化实验，每个实验都可能包含巨量计算。

**多任务训练**，将几个到几十个相关任务的适当规模同时训练，或者从针对相关任务的大量数据训练的模型中迁移学习然后针对新任务在少量数据上进行微调，这些方式已被证明在解决各类问题时都非常有效。

深度学习模型缺乏透明度和足够的**可解释性**，从而在一定程度上限制了它在严肃科学领域的应用。



在传统学习理论中，模型的参数越多，模型一般会更好地拟合训练数据，但模型的泛化能力（拟合测试数据集的能力）会变差。在深度学习中，参数的数目比训练数据集要大得多，但深度网络（DNN）却通常既能拟合好训练数据，又保持良好的泛化能力。**这个违反直觉的现象被大家称为“明显悖论” (apparent paradox)。**



频率原则可以粗糙地表述成：**DNN 在拟合目标函数的过程中，有从低频到高频的先后顺序。**

如果让一个人去记住一个没见过的东西，一般比较粗糙的轮廓信息会先被记住，然后再是很多细节。没错，DNN 也正是使用了这样的一个学习过程。



随着机器学习领域的快速发展和数据量的急剧增加(大数据时代)，数据驱动的方法变得越来越流行。机器学习算法可以仅使用数据去分析科学问题。

机器学习已经引起了科学方法的根本转变。 传统上，科学研究围绕理论和实验展开：先设计一个定义明确的理论，然后使用实验数据不断地对其进行改进，并对其进行分析以做出新的预测。



Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, and integral-differential equations. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages, the review also attempts to incorporate publications on a larger variety of issues, including physics-constrained neural networks (PCNN), where the initial or boundary conditions are directly embedded in the NN structure rather than in the loss functions. The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.



物理信息神经网络 (PINN)求解(含有噪声的离散数据的)整数阶偏微分方程 (PDE) 是有效的。PINN 采用标准前馈神经网络 (NN)，其中 PDE 使用自动微分显式编码到 NN 中，而初始边界条件下的均方 PDE 残差和均方误差之和相对于 NN 参数最小化. 在这里，我们将 PINN 扩展到分数 PINN（fPINN）以求解时空分数平流扩散方程（分数 ADE），并且我们系统地研究了它们的收敛性，从而首次解释了 fPINN 和 PINN。具体来说，*黑盒子*(BB) 强制条款）。fPINNs 的一个新元素是我们引入的混合方法，它使用整数阶算子的自动微分和分数算子的数值离散化来构造损失函数中的残差。这种方法绕过了自动微分不适用于分数运算符这一事实所带来的困难，因为整数微积分中的标准链式法则在分数微积分中无效。为了离散分数运算符，我们在一维分数 ADE 中使用 Grünwald-Letnikov (GL) 公式，在二维和三维分数 ADE 中使用向量 GL 公式和方向分数拉普拉斯算子。我们首先考虑一维分数泊松方程，并将 fPINN 的收敛性与有限差分法 (FDM) 进行比较。由于对 NN 参数初始化的敏感性，我们使用平均 $L^2$ 误差和标准偏差来呈现解决方案收敛性。使用不同的 GL 公式，我们观察到小型训练集的一阶、二阶和三阶收敛速度，但对于较大的训练集，误差会饱和。我们通过分析离散化、采样、NN 逼近和优化导致的四个数值误差来源来解释这些结果。总误差单调衰减（对于三阶 GL 公式，低于 $10^{-5}$），但由于优化误差，超过该点会饱和。*辅助点*) 应具有可比性以实现最高准确度。当我们将 NN 的深度增加到某个值时，平均误差会减小，标准差会增加，而宽度基本上没有影响，除非它的值太小或太大。我们接下来考虑与时间相关的分数 ADE，并比较白盒 (WB) 和 BB 强迫。我们观察到，对于 WB 强制，我们的结果与上述情况相似；然而，对于 BB，强制 fPINN 的性能优于 FDM。随后，我们使用方向分数拉普拉斯算子考虑多维时间、空间和时空分数 ADE，我们观察到 $10^{-3}\sim10^{-4}$ 的相对误差。最后，我们解决了几个一维、二维和三维的逆问题，以识别分数阶、扩散系数、

Physics-informed neural networks (PINNs), introduced in [M. Raissi, P. Perdikaris, and G. Karniadakis, *J. Comput. Phys.*, 378 (2019), pp. 686--707], are effective in solving integer-order partial differential equations (PDEs) based on scattered and noisy data. PINNs employ standard feedforward neural networks (NNs) with the PDEs explicitly encoded into the NN using automatic differentiation, while the sum of the mean-squared PDE residuals and the mean-squared error in initial-boundary conditions is minimized with respect to the NN parameters. Here we extend PINNs to fractional PINNs (fPINNs) to solve space-time fractional advection-diffusion equations (fractional ADEs), and we study systematically their convergence, hence explaining both fPINNs and PINNs for the first time. Specifically, we demonstrate their accuracy and effectiveness in solving multidimensional forward and inverse problems with forcing terms whose values are only known at randomly scattered spatio-temporal coordinates (*black-box* (BB) forcing terms). A novel element of the fPINNs is the hybrid approach that we introduce for constructing the residual in the loss function using both automatic differentiation for the integer-order operators and numerical discretization for the fractional operators. This approach bypasses the difficulties stemming from the fact that automatic differentiation is not applicable to fractional operators because the standard chain rule in integer calculus is not valid in fractional calculus. To discretize the fractional operators, we employ the Grünwald--Letnikov (GL) formula in one-dimensional fractional ADEs and the vector GL formula in conjunction with the directional fractional Laplacian in two- and three-dimensional fractional ADEs. We first consider the one-dimensional fractional Poisson equation and compare the convergence of the fPINNs against the finite difference method (FDM). We present the solution convergence using both the mean $L^2$ error as well as the standard deviation due to sensitivity to NN parameter initializations. Using different GL formulas we observe first-, second-, and third-order convergence rates for small size training sets but the error saturates for larger training sets. We explain these results by analyzing the four sources of numerical errors due to discretization, sampling, NN approximation, and optimization. The total error decays monotonically (below $10^{-5}$ for a third-order GL formula) but it saturates beyond that point due to the optimization error. We also analyze the relative balance between discretization and sampling errors and observe that the sampling size and the number of discretization points (*auxiliary points*) should be comparable to achieve the highest accuracy. As we increase the depth of the NN up to certain value, the mean error decreases and the standard deviation increases, whereas the width has essentially no effect unless its value is either too small or too large. We next consider time-dependent fractional ADEs and compare white-box (WB) and BB forcing. We observe that for the WB forcing, our results are similar to the aforementioned cases; however, for the BB forcing fPINNs outperform FDM. Subsequently, we consider multidimensional time-, space-, and space-time-fractional ADEs using the directional fractional Laplacian and we observe relative errors of $10^{-3}\sim10^{-4}$. Finally, we solve several inverse problems in one, two, and three dimensions to identify the fractional orders, diffusion coefficients, and transport velocities and obtain accurate results given proper initializations even in the presence of significant noise.



### 问题设置

 Fractional advection-diffusion equations(fractional ADE)
 分数阶对流扩散方程

来自论文：https://epubs.siam.org/doi/epdf/10.1137/18M1229845
$$
\large
\begin{aligned}
\frac{\partial^{\gamma} u(\boldsymbol{x},t)}{\partial t^{\gamma}} &= -c(-\Delta)^{\alpha/2} u(\boldsymbol{x},t) - \boldsymbol{v}\cdot \nabla u(\boldsymbol{x},t) + f_{BB}(\boldsymbol{x},t), \boldsymbol{x}\in\Omega \subset \mathbb{R}^D,t\in(0,T],\\

u(\boldsymbol{x},t) &= 0, \boldsymbol{x}\in \partial \Omega,\\

u(\boldsymbol{x},0) &= g(x), \boldsymbol{x}\in \Omega.\\
\end{aligned}
\tag{2.1}
$$
也假设解 $u(\boldsymbol{x},t)$ 在 $\Omega$ 的外部为零。上述方程的左侧是伽马阶的时间分数导数，它是在Caputo意义下定义的。
$$
\large 
\frac{\partial^{\gamma} u(\boldsymbol{x},t)}{\partial t^{\gamma}}
=\frac{1}{\Gamma(1-\gamma)} \int_0^t (t-\tau)^{-\gamma} \frac{\partial u(\boldsymbol{x},t)}{\partial t}d\tau , 0<\gamma<1, \\
$$
其中 $\Gamma(\cdot)$ 是Gamma函数。当 $\gamma\rightarrow 1$ 时，时间--分数阶导数还原为一阶导数。右侧的第一项是分数拉普拉斯，它是在方向导数的意义上定义的。
$$
\large 
(-\Delta)^{\alpha/2} u(\boldsymbol{x},t) = \frac{\Gamma(\frac{1-\alpha}{2})\Gamma(\frac{D+\alpha}{2})}{2\pi^{\frac{D+1}{2}}} \int_{||\boldsymbol{\theta}||_2=1} D_{\boldsymbol{\theta}}^{\alpha} u(\boldsymbol{x},t) d\boldsymbol{\theta}, \boldsymbol{\theta} \in \mathbb{R}^D, 1<\alpha<2,\\
$$
其中$|| \cdot ||_2$是向量的L2范数。$D_{\boldsymbol{\theta}}^{\alpha}$表示方向分数微分算子，其中$\boldsymbol{\theta}$是微分方向向量。第3.3节将给出该运算符及其离散化的视图。正如α向右箭头2所示，分数拉普拉斯阶（2.3）减少到标准拉普拉斯阶“$-\Delta$”。在问题（2.1）中，$\boldsymbol{v}$是平均流速，$f_{BB}$是BB强迫项，其值仅在分散的时空坐标中已知。在考虑地下水污染物迁移中的应用时，分数阶γ和α分别限制为（0,1）和（1,2）。此外，为了简单起见，我们考虑零边界条件。

前面的问题公式如下：给定分数阶α和γ、扩散系数c、流速$\boldsymbol{v}$、BB强迫项$f_{BB}$以及初始和边界条件，我们解决了浓度场$u(\boldsymbol{x},t)$的问题（2.1）。另一方面，反问题定义如下：给定初始边界条件、BB强迫项$f_{BB}$和最终时间$u(\boldsymbol{x},t) = h_{BB}(\boldsymbol{x}, T)$的额外浓度测量，我们解决了分数阶α和γ、扩散系数c、流速$\boldsymbol{v}$和浓度场$u(\boldsymbol{x},t)$的问题（2.1）。我们也可以在时间上考虑分散的测量，但在这里，我们调查的是一个更具挑战性的案例，在最后阶段只有可用的数据。







# 深度算子网络 DeepONet



众所周知，神经网络 (NN) 是连续函数的通用逼近器。然而，一个鲜为人知但强大的结果是具有单个隐藏层的 NN 可以准确地逼近任何非线性连续算子。算子的这种通用逼近定理暗示了深度神经网络 (DNN) 在从分散的数据流中学习连续算子或复杂系统的结构和潜力。在这里，我们因此将这个定理扩展到 DNN。我们设计了一个泛化误差小的新网络，即深度算子网络（DeepONet），它由一个用于编码离散输入函数空间（分支网络）的 DNN 和另一个用于编码输出函数域（主干网络）的 DNN 组成。我们证明 DeepONet 可以学习各种显式运算符，例如积分和分数拉普拉斯算子，以及表示确定性和随机微分方程的隐式算子。

It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.



常见的算子有微分算子，梯度算子，散度算子，拉普拉斯算子，哈密顿算子等。

狭义的算子实际上是指从一个函数空间到另一个函数空间（或它自身）的映射。

广义的算子的定义只要把上面的空间推广到一般空间，可以是向量空间。赋范向量空间，内积空间，或更进一步，Banach空间，Hilbert空间都可以。算子还可分为有界的与无界的，线性的与非线性的等等类别。



广义的讲，对任何函数进行某一项操作都可以认为是一个算子，甚至包括求幂次，开方都可以认为是一个算子，只是有的算子我们用了一个符号来代替他所要进行的运算罢了，所以大家看到算子就不要纠结，他和 $f(x)$ 的 $f$ 没区别，它甚至和加减乘除的基本运算符号都没有区别，只是他可以对单对象操作罢了(有的符号比如大于、小于号要对多对象操作)。



[a,b]上定义的连续函数的全体势为 $\aleph^1$，[a,b]上定义的任意函数的全体势为 $\aleph^2$，







最近，基于物理的神经网络 (PINN) 为解决与微分方程相关的问题提供了一种强大的新范式。与经典数值方法相比，PINN 具有几个优点，例如它们能够提供微分方程的无网格解以及它们在同一优化问题中执行正向和逆向建模的能力。虽然很有希望，但迄今为止的一个关键限制是 PINN 一直在努力准确有效地解决大域和/或多尺度解决方案的问题，这对于它们的实际应用至关重要。多个重要的相关因素导致了这个问题，包括随着问题规模的增长和神经网络的频谱偏差，底层 PINN 优化问题的复杂性不断增加。在这项工作中，我们提出了一种新的、可扩展的方法来解决与微分方程相关的大型问题，称为有限基 PINN (FBPINN)。FBPINN 受到经典有限元方法的启发，其中微分方程的解表示为具有紧支持的有限基函数集的总和。在 FBPINN 中，神经网络用于学习这些基函数，这些基函数是在小的重叠子域上定义的。FBINN 旨在通过在每个子域上使用单独的输入归一化来解决神经网络的频谱偏差，并通过在并行分而治之的方法中使用许多较小的神经网络来降低底层优化问题的复杂性。我们的数值实验表明，FBPINN 可以有效地解决小型和大型的多尺度问题，



Recently, physics-informed neural networks (PINNs) have offered a powerful new paradigm for solving problems relating to differential equations. Compared to classical numerical methods PINNs have several advantages, for example their ability to provide mesh-free solutions of differential equations and their ability to carry out forward and inverse modelling within the same optimisation problem. Whilst promising, a key limitation to date is that PINNs have struggled to accurately and efficiently solve problems with large domains and/or multi-scale solutions, which is crucial for their real-world application. Multiple significant and related factors contribute to this issue, including the increasing complexity of the underlying PINN optimisation problem as the problem size grows and the spectral bias of neural networks. In this work we propose a new, scalable approach for solving large problems relating to differential equations called Finite Basis PINNs (FBPINNs). FBPINNs are inspired by classical finite element methods, where the solution of the differential equation is expressed as the sum of a finite set of basis functions with compact support. In FBPINNs neural networks are used to learn these basis functions, which are defined over small, overlapping subdomains. FBINNs are designed to address the spectral bias of neural networks by using separate input normalisation over each subdomain, and reduce the complexity of the underlying optimisation problem by using many smaller neural networks in a parallel divide-and-conquer approach. Our numerical experiments show that FBPINNs are effective in solving both small and larger, multi-scale problems, outperforming standard PINNs in both accuracy and computational resources required, potentially paving the way to the application of PINNs on large, real-world problems.













